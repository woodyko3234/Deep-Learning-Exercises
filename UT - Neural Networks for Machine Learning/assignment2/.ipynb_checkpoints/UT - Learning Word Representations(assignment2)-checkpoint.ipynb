{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "data = loadmat('data.mat')['data']\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('testData', 'O'), ('trainData', 'O'), ('validData', 'O'), ('vocab', 'O')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4\n"
     ]
    }
   ],
   "source": [
    "trainData = data['trainData'][0][0]\n",
    "validData = data['validData'][0][0]\n",
    "testData = data['testData'][0][0]\n",
    "print(len(trainData), len(validData), len(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 372550), (4, 46568), (4, 46568))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.shape, validData.shape, testData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['all'], dtype='<U3') array(['set'], dtype='<U3')\n",
      " array(['just'], dtype='<U4') array(['show'], dtype='<U4')\n",
      " array(['being'], dtype='<U5')]\n",
      "[27 25 89] [143]\n",
      "[169 136 189] [142]\n",
      "[182  89 186] [143]\n"
     ]
    }
   ],
   "source": [
    "#setting for later processes\n",
    "vocab = data['vocab'][0][0][0]\n",
    "print(vocab[:5])\n",
    "train_input = trainData[0:3,:] -1\n",
    "train_target = trainData[3,:].reshape((1,-1)) -1\n",
    "print(train_input[:,0], train_target[:,0])\n",
    "valid_input = validData[0:3,:] -1\n",
    "valid_target = validData[3,:].reshape((1,-1)) -1\n",
    "print(valid_input[:,0], valid_target[:,0])\n",
    "test_input = testData[0:3,:] -1\n",
    "test_target = testData[3,:].reshape((1,-1)) -1\n",
    "print(test_input[:,0], test_target[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Octave starts from 1, while python starts from 0!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIALIZE WEIGHTS AND BIASES.\n",
    "vocab_size = len(vocab) # vocab size\n",
    "batchsize = 100  # Mini-batch size.\n",
    "learning_rate = 0.1  # Learning rate; default = 0.1.\n",
    "momentum = 0.9  # Momentum; default = 0.9.\n",
    "numhid1 = 50    # Dimensionality of embedding space; default = 50.\n",
    "numhid2 = 200   # Number of units in hidden layer; default = 200.\n",
    "init_wt = 0.01  # Standard deviation of the normal distribution, \n",
    "                # which is sampled to get the initial weights; default = 0.01\n",
    "numwords = train_input.shape[0]\n",
    "np.random.seed(32)\n",
    "word_embedding_weights = init_wt * np.random.randn(vocab_size, numhid1)\n",
    "embed_to_hid_weights = init_wt * np.random.randn(numhid1*numwords, numhid2)\n",
    "hid_to_output_weights = init_wt * np.random.randn(numhid2, vocab_size)\n",
    "hid_bias = np.zeros((numhid2, 1))\n",
    "output_bias = np.zeros((vocab_size, 1))\n",
    "\n",
    "word_embedding_weights_delta = np.zeros((vocab_size, numhid1))\n",
    "word_embedding_weights_gradient = np.zeros((vocab_size, numhid1))\n",
    "embed_to_hid_weights_delta = np.zeros((numhid1*numwords, numhid2))\n",
    "hid_to_output_weights_delta = np.zeros((numhid2, vocab_size))\n",
    "hid_bias_delta = np.zeros((numhid2, 1))\n",
    "output_bias_delta = np.zeros((vocab_size, 1))\n",
    "expansion_matrix = np.eye(vocab_size) # matrix which outputs 1 if i = j else 0\n",
    "count = 0\n",
    "tiny = np.exp(-30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fprop(input_batch, word_embedding_weights, embed_to_hid_weights, hid_to_output_weights, hid_bias, output_bias):\n",
    "    '''\n",
    "    This method forward propagates through a neural network.\n",
    "    Inputs:\n",
    "        input_batch: The input data as a matrix of size numwords X batchsize where,\n",
    "                     numwords is the number of words, batchsize is the number of data points.\n",
    "                     So, if input_batch(i, j) = k then the ith word in data point j is word\n",
    "                     index k of the vocabulary.\n",
    "\n",
    "        word_embedding_weights: Word embedding as a matrix of size\n",
    "                                vocab_size X numhid1, where vocab_size is the size of the vocabulary\n",
    "                                numhid1 is the dimensionality of the embedding space.\n",
    "\n",
    "        embed_to_hid_weights: Weights between the word embedding layer and hidden\n",
    "                              layer as a matrix of soze numhid1*numwords X numhid2, numhid2 is the\n",
    "                              number of hidden units.\n",
    "\n",
    "        hid_to_output_weights: Weights between the hidden layer and output softmax\n",
    "                               unit as a matrix of size numhid2 X vocab_size\n",
    "\n",
    "        hid_bias: Bias of the hidden layer as a matrix of size numhid2 X 1.\n",
    "\n",
    "        output_bias: Bias of the output layer as a matrix of size vocab_size X 1.\n",
    "\n",
    "    Outputs:\n",
    "        embedding_layer_state: State of units in the embedding layer as a matrix of\n",
    "                               size numhid1*numwords X batchsize\n",
    "\n",
    "        hidden_layer_state: State of units in the hidden layer as a matrix of size\n",
    "                            numhid2 X batchsize\n",
    "\n",
    "        output_layer_state: State of units in the output layer as a matrix of size\n",
    "                            vocab_size X batchsize\n",
    "    '''\n",
    "\n",
    "    numwords, batchsize = input_batch.shape\n",
    "    vocab_size, numhid1 = word_embedding_weights.shape\n",
    "    numhid2 = embed_to_hid_weights.shape[1]\n",
    "\n",
    "    ## COMPUTE STATE OF WORD EMBEDDING LAYER.\n",
    "    # Look up the inputs word indices in the word_embedding_weights matrix.\n",
    "    #embedding_layer_state = reshape(word_embedding_weights(reshape(input_batch, 1, []),:).T, numhid1 * numwords, [])\n",
    "    embedding_layer_state = word_embedding_weights[input_batch.reshape((1,-1)), :].T.reshape((numhid1 * numwords, -1))\n",
    "    \n",
    "    ## COMPUTE STATE OF HIDDEN LAYER.\n",
    "    # Compute inputs to hidden units.\n",
    "    inputs_to_hidden_units = np.dot(embed_to_hid_weights.T, embedding_layer_state) + hid_bias\n",
    "\n",
    "    # Apply logistic activation function.\n",
    "    # FILL IN CODE. Replace the line below by one of the options.\n",
    "    #hidden_layer_state = zeros(numhid2, batchsize);\n",
    "    # Options\n",
    "    # (a) hidden_layer_state = 1 ./ (1 + exp(inputs_to_hidden_units));\n",
    "    # (b) hidden_layer_state = 1 ./ (1 - exp(-inputs_to_hidden_units));\n",
    "    hidden_layer_state = 1. / (1 + np.exp(-inputs_to_hidden_units))\n",
    "    # (d) hidden_layer_state = -1 ./ (1 + exp(-inputs_to_hidden_units));\n",
    "\n",
    "    ## COMPUTE STATE OF OUTPUT LAYER.\n",
    "    # Compute inputs to softmax.\n",
    "    # FILL IN CODE. Replace the line below by one of the options.\n",
    "    #inputs_to_softmax = zeros(vocab_size, batchsize);\n",
    "    # Options\n",
    "    inputs_to_softmax = np.dot(hid_to_output_weights.T, hidden_layer_state) + output_bias\n",
    "    # (b) inputs_to_softmax = hid_to_output_weights' * hidden_layer_state +  repmat(output_bias, batchsize, 1);\n",
    "    # (c) inputs_to_softmax = hidden_layer_state * hid_to_output_weights' +  repmat(output_bias, 1, batchsize);\n",
    "    # (d) inputs_to_softmax = hid_to_output_weights * hidden_layer_state +  repmat(output_bias, batchsize, 1);\n",
    "\n",
    "    # Subtract maximum. \n",
    "    # Remember that adding or subtracting the same constant from each input to a\n",
    "    # softmax unit does not affect the outputs. Here we are subtracting maximum to\n",
    "    # make all inputs <= 0. This prevents overflows when computing their\n",
    "    # exponents.\n",
    "    inputs_to_softmax = inputs_to_softmax - np.max(inputs_to_softmax, axis = 0)\n",
    "\n",
    "    # Compute exp.\n",
    "    output_layer_state = np.exp(inputs_to_softmax)\n",
    "\n",
    "    # Normalize to get probability distribution.\n",
    "    output_layer_state = output_layer_state / np.sum(output_layer_state, axis = 0)\n",
    "\n",
    "    \n",
    "    return embedding_layer_state, hidden_layer_state, output_layer_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, train_input, train_target, word_embedding_weights, embed_to_hid_weights,\n",
    "          hid_to_output_weights, hid_bias, output_bias, valid_input, valid_target, vocab,\n",
    "          word_embedding_weights_delta, embed_to_hid_weights_delta, hid_to_output_weights_delta,\n",
    "          hid_bias_delta, output_bias_delta,\n",
    "          vocab_size = 250, \n",
    "          batchsize = 100, learning_rate = 0.1, numhid1 = 50, numhid2 = 200,\n",
    "          count = 0, momentum = 0.9, tiny = np.exp(-30)):\n",
    "    \n",
    "    expansion_matrix = np.eye(vocab_size)\n",
    "    numwords = train_input.shape[0]\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch %d\\n' % epoch)\n",
    "        this_chunk_CE = 0\n",
    "        trainset_CE = 0\n",
    "        # LOOP OVER MINI-BATCHES.\n",
    "        for m in range(len(train_target[0])//batchsize + 1):\n",
    "            #print('Batch%d' % m)\n",
    "            if m < len(train_target[0])//batchsize:\n",
    "                input_batch = train_input[:, m*batchsize:(m+1)*batchsize]\n",
    "                target_batch = train_target[:, m*batchsize:(m+1)*batchsize]\n",
    "            else:\n",
    "                input_batch = train_input[:, m*batchsize:]\n",
    "                target_batch = train_target[:, m*batchsize:]\n",
    "\n",
    "            # FORWARD PROPAGATE.\n",
    "            # Compute the state of each layer in the network given the input batch\n",
    "            # and all weights and biases\n",
    "            embedding_layer_state, hidden_layer_state, output_layer_state = fprop(input_batch, \n",
    "                                                                                    word_embedding_weights, \n",
    "                                                                                    embed_to_hid_weights, \n",
    "                                                                                    hid_to_output_weights, \n",
    "                                                                                    hid_bias, output_bias)\n",
    "\n",
    "            # COMPUTE DERIVATIVE.\n",
    "            # Expand the target to a sparse 1-of-K vector.\n",
    "            #print(target_batch.ravel().max())\n",
    "            expanded_target_batch = expansion_matrix[:, target_batch.ravel()]\n",
    "            # Compute derivative of cross-entropy loss function.\n",
    "            error_deriv = output_layer_state - expanded_target_batch\n",
    "            #print(error_deriv.shape, output_layer_state.shape, expanded_target_batch.shape)\n",
    "\n",
    "            # MEASURE LOSS FUNCTION.\n",
    "            cross_entropy = -np.sum(np.sum(expanded_target_batch * np.log(output_layer_state + tiny))) / batchsize\n",
    "            count += 1\n",
    "            this_chunk_CE = this_chunk_CE + (cross_entropy - this_chunk_CE) / count\n",
    "            trainset_CE = trainset_CE + (cross_entropy - trainset_CE) / m\n",
    "            if m % 1000 == 0:\n",
    "                print('\\rBatch %d Train CE %.3f' % (m, this_chunk_CE))\n",
    "    \n",
    "\n",
    "            # BACK PROPAGATE.\n",
    "            ## OUTPUT LAYER.\n",
    "            hid_to_output_weights_gradient =  hidden_layer_state.dot(error_deriv.T)\n",
    "            output_bias_gradient = np.sum(error_deriv, axis = 1, keepdims = True)\n",
    "            back_propagated_deriv_1 = (hid_to_output_weights.dot(error_deriv)) * hidden_layer_state * (1 - hidden_layer_state)\n",
    "\n",
    "            ## HIDDEN LAYER.\n",
    "            # FILL IN CODE. Replace the line below by one of the options.\n",
    "            #embed_to_hid_weights_gradient = zeros(numhid1 * numwords, numhid2)\n",
    "            # Options:\n",
    "            # (a) embed_to_hid_weights_gradient = back_propagated_deriv_1' * embedding_layer_state;\n",
    "            embed_to_hid_weights_gradient = embedding_layer_state.dot(back_propagated_deriv_1.T)\n",
    "            # (c) embed_to_hid_weights_gradient = back_propagated_deriv_1;\n",
    "            # (d) embed_to_hid_weights_gradient = embedding_layer_state;\n",
    "\n",
    "            # FILL IN CODE. Replace the line below by one of the options.\n",
    "            #hid_bias_gradient = zeros(numhid2, 1)\n",
    "            # Options\n",
    "            hid_bias_gradient = np.sum(back_propagated_deriv_1, axis = 1, keepdims = True)\n",
    "            # (b) hid_bias_gradient = sum(back_propagated_deriv_1, 1);\n",
    "            # (c) hid_bias_gradient = back_propagated_deriv_1;\n",
    "            # (d) hid_bias_gradient = back_propagated_deriv_1';\n",
    "\n",
    "            # FILL IN CODE. Replace the line below by one of the options.\n",
    "            #back_propagated_deriv_2 = zeros(numhid2, batchsize);\n",
    "            # Options\n",
    "            back_propagated_deriv_2 = embed_to_hid_weights.dot(back_propagated_deriv_1)\n",
    "            # (b) back_propagated_deriv_2 = back_propagated_deriv_1 * embed_to_hid_weights;\n",
    "            # (c) back_propagated_deriv_2 = back_propagated_deriv_1' * embed_to_hid_weights;\n",
    "            # (d) back_propagated_deriv_2 = back_propagated_deriv_1 * embed_to_hid_weights';\n",
    "\n",
    "            word_embedding_weights_gradient = np.zeros((vocab_size, numhid1))\n",
    "            ## EMBEDDING LAYER.\n",
    "            for w in range(numwords):\n",
    "                word_embedding_weights_gradient = (word_embedding_weights_gradient + \n",
    "                                                   (expansion_matrix[:, input_batch[w, :].ravel()].dot(\n",
    "                                                       back_propagated_deriv_2[w * numhid1 : (w+1) * numhid1, :].T)))\n",
    "    \n",
    "            # UPDATE WEIGHTS AND BIASES.\n",
    "            word_embedding_weights_delta = (momentum * word_embedding_weights_delta + \n",
    "                                            word_embedding_weights_gradient / batchsize)\n",
    "            word_embedding_weights -= learning_rate * word_embedding_weights_delta\n",
    "\n",
    "            embed_to_hid_weights_delta = (momentum * embed_to_hid_weights_delta + \n",
    "                                          embed_to_hid_weights_gradient / batchsize)\n",
    "            embed_to_hid_weights -= learning_rate * embed_to_hid_weights_delta\n",
    "\n",
    "            hid_to_output_weights_delta = (momentum * hid_to_output_weights_delta + \n",
    "                                           hid_to_output_weights_gradient / batchsize)\n",
    "            hid_to_output_weights -= learning_rate * hid_to_output_weights_delta\n",
    "\n",
    "            hid_bias_delta = (momentum * hid_bias_delta + hid_bias_gradient / batchsize)\n",
    "            hid_bias -= learning_rate * hid_bias_delta\n",
    "\n",
    "            output_bias_delta = (momentum * output_bias_delta + output_bias_gradient / batchsize)\n",
    "            output_bias -= learning_rate * output_bias_delta\n",
    "\n",
    "            \n",
    "        print('\\rAverage Training CE %.3f\\n' % trainset_CE)\n",
    "        \n",
    "        print('Finished 1 epoch Training.\\n')\n",
    "        \n",
    "        # VALIDATE.\n",
    "        print('\\rRunning validation ...')\n",
    "        embedding_layer_state, hidden_layer_state, output_layer_state = fprop(valid_input, \n",
    "                                                                              word_embedding_weights, \n",
    "                                                                              embed_to_hid_weights,\n",
    "                                                                              hid_to_output_weights, \n",
    "                                                                              hid_bias, output_bias)\n",
    "        datasetsize = len(valid_input[0]);\n",
    "        expanded_valid_target = expansion_matrix[:, valid_target.ravel()]\n",
    "        CE = -np.sum(np.sum(expanded_valid_target * np.log(output_layer_state + tiny))) / datasetsize\n",
    "        print(' Validation CE %.3f\\n' % CE)\n",
    "    print('Final Training CE %.3f\\n', trainset_CE)\n",
    "    \n",
    "    model = dict()\n",
    "    model['word_embedding_weights'] = word_embedding_weights\n",
    "    model['embed_to_hid_weights'] = embed_to_hid_weights\n",
    "    model['hid_to_output_weights'] = hid_to_output_weights\n",
    "    model['hid_bias'] = hid_bias\n",
    "    model['output_bias'] = output_bias\n",
    "    model['vocab'] = vocab\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('Training took %.2f seconds' % (end_time - start_time))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\n",
      "\r",
      "Batch 0 Train CE 3.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KunWuYao/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:47: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "/Users/KunWuYao/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:47: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/KunWuYao/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:55: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000 Train CE 3.150\n",
      "Batch 2000 Train CE 3.107\n",
      "Batch 3000 Train CE 3.083\n",
      "Average Training CE nan\n",
      "\n",
      "Finished 1 epoch Training.\n",
      "\n",
      "Running validation ...\n",
      " Validation CE 3.017\n",
      "\n",
      "Epoch 1\n",
      "\n",
      "Batch 0 Train CE 0.001\n",
      "Batch 1000 Train CE 0.635\n",
      "Batch 2000 Train CE 1.045\n",
      "Batch 3000 Train CE 1.332\n",
      "Average Training CE nan\n",
      "\n",
      "Finished 1 epoch Training.\n",
      "\n",
      "Running validation ...\n",
      " Validation CE 2.977\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "Batch 0 Train CE 0.000\n",
      "Batch 1000 Train CE 0.350\n",
      "Batch 2000 Train CE 0.625\n",
      "Batch 3000 Train CE 0.847\n",
      "Average Training CE nan\n",
      "\n",
      "Finished 1 epoch Training.\n",
      "\n",
      "Running validation ...\n",
      " Validation CE 2.955\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "Batch 0 Train CE 0.000\n",
      "Batch 1000 Train CE 0.241\n",
      "Batch 2000 Train CE 0.444\n",
      "Batch 3000 Train CE 0.619\n",
      "Average Training CE nan\n",
      "\n",
      "Finished 1 epoch Training.\n",
      "\n",
      "Running validation ...\n",
      " Validation CE 2.937\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "Batch 0 Train CE 0.000\n",
      "Batch 1000 Train CE 0.183\n",
      "Batch 2000 Train CE 0.344\n",
      "Batch 3000 Train CE 0.487\n",
      "Average Training CE nan\n",
      "\n",
      "Finished 1 epoch Training.\n",
      "\n",
      "Running validation ...\n",
      " Validation CE 2.924\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "Batch 0 Train CE 0.000\n",
      "Batch 1000 Train CE 0.148\n",
      "Batch 2000 Train CE 0.280\n",
      "Batch 3000 Train CE 0.401\n",
      "Average Training CE nan\n",
      "\n",
      "Finished 1 epoch Training.\n",
      "\n",
      "Running validation ...\n",
      " Validation CE 2.914\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "Batch 0 Train CE 0.000\n",
      "Batch 1000 Train CE 0.123\n",
      "Batch 2000 Train CE 0.236\n",
      "Batch 3000 Train CE 0.340\n",
      "Average Training CE nan\n",
      "\n",
      "Finished 1 epoch Training.\n",
      "\n",
      "Running validation ...\n",
      " Validation CE 2.904\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "Batch 0 Train CE 0.000\n",
      "Batch 1000 Train CE 0.106\n",
      "Batch 2000 Train CE 0.204\n",
      "Batch 3000 Train CE 0.296\n",
      "Average Training CE nan\n",
      "\n",
      "Finished 1 epoch Training.\n",
      "\n",
      "Running validation ...\n",
      " Validation CE 2.896\n",
      "\n",
      "Epoch 8\n",
      "\n",
      "Batch 0 Train CE 0.000\n",
      "Batch 1000 Train CE 0.093\n",
      "Batch 2000 Train CE 0.180\n",
      "Batch 3000 Train CE 0.261\n",
      "Average Training CE nan\n",
      "\n",
      "Finished 1 epoch Training.\n",
      "\n",
      "Running validation ...\n",
      " Validation CE 2.889\n",
      "\n",
      "Epoch 9\n",
      "\n",
      "Batch 0 Train CE 0.000\n",
      "Batch 1000 Train CE 0.083\n",
      "Batch 2000 Train CE 0.160\n",
      "Batch 3000 Train CE 0.234\n",
      "Average Training CE nan\n",
      "\n",
      "Finished 1 epoch Training.\n",
      "\n",
      "Running validation ...\n",
      " Validation CE 2.884\n",
      "\n",
      "Final Training CE %.3f\n",
      " nan\n",
      "Training took 193.83 seconds\n"
     ]
    }
   ],
   "source": [
    "model = train(10, train_input, train_target, word_embedding_weights, embed_to_hid_weights,\n",
    "          hid_to_output_weights, hid_bias, output_bias, valid_input, valid_target, vocab,\n",
    "          word_embedding_weights_delta, embed_to_hid_weights_delta, hid_to_output_weights_delta,\n",
    "          hid_bias_delta, output_bias_delta,\n",
    "          vocab_size = 250, \n",
    "          batchsize = 100, learning_rate = 0.1, numhid1 = 50, numhid2 = 200,\n",
    "          count = 0, momentum = 0.9, tiny = np.exp(-30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run test\n",
    "def test(model, test_input, test_target):\n",
    "    word_embedding_weights = model['word_embedding_weights']\n",
    "    embed_to_hid_weights = model['embed_to_hid_weights']\n",
    "    hid_to_output_weights = model['hid_to_output_weights']\n",
    "    hid_bias = model['hid_bias']\n",
    "    output_bias = model['output_bias']\n",
    "    \n",
    "    embedding_layer_state, hidden_layer_state, output_layer_state = fprop(test_input, \n",
    "                                                                          word_embedding_weights, \n",
    "                                                                          embed_to_hid_weights,\n",
    "                                                                          hid_to_output_weights, \n",
    "                                                                          hid_bias, output_bias)\n",
    "    datasetsize = len(test_input[0])\n",
    "    expanded_test_target = expansion_matrix[:, test_target.ravel()]\n",
    "    CE = -np.sum(np.sum(expanded_test_target * np.log(output_layer_state + tiny))) / datasetsize\n",
    "    print(1, '\\rFinal Test CE %.3f\\n', CE)\n",
    "    return CE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/KunWuYao/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:55: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \r",
      "Final Test CE %.3f\n",
      " 2.8896407304431304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.8896407304431304"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_input, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
